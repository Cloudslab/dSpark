#Number of Worker Nodes
worker.numbers=1

###Configuration of each Worker Machine

#total usable cpu cores in each worker ** can be taken from spark-env.sh file in future
worker.cores=6

#total usable memory in each worker (in GB) ** can be taken from spark-env.sh file in future
worker.memory=6

#Maximum limit of cpu cores per executor (it will be 5 for now as recommended by Spark developers)

executor.cores.limit=5

#Input Size for Profiling (in GB)

profiler.input.size=1

#Algorithm to be used with profiler
profiler.algorithm=wordcount

#Profiling Level (1. high = use all possible combination of cores for executors(1~5), it will take a lot of time to profile;
# 				  2. low = only use default cores limit(5) per executor)

profiler.level=high
#Path for Spark Home Directory
spark.home=/home/tawfiq/sp/spark-2.0.1

#Path for Profiler Input File
profiler.input.path=/home/tawfiq/sp/spark-2.0.1/myinput

#Path for Application Input File
application.input.path=/home/tawfiq/sp/spark-2.0.1/myappinput

#Path for Application Jar File
application.jar.path=/home/tawfiq/sp/spark-2.0.1/bigdatabench-spark_1.3.0-hadoop_1.0.4.jar

#Class to use of Application
application.class=cn.ac.ict.bigdatabench.WordCount

#path to store output of Application
application.outputPath=/home/tawfiq/sp/spark-2.0.1/myoutput



